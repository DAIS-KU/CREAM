import json
import os
import math


def generate_query_and_docs(input_file, queries_file, docs_file):
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    with open(queries_file, "w", encoding="utf-8") as qf, open(
        docs_file, "w", encoding="utf-8"
    ) as df:
        for item in data:
            query_id = item["query_id"]
            question = item["question"]
            context = item["context"]

            # queries.jsonlìš© ë ˆì½”ë“œ
            query_obj = {
                "qid": query_id,
                "query": question,
                "answer_pids": [f"doc_{query_id}"],
            }
            qf.write(json.dumps(query_obj, ensure_ascii=False) + "\n")

            # docs.jsonlìš© ë ˆì½”ë“œ
            doc_obj = {"doc_id": f"doc_{query_id}", "text": context}
            df.write(json.dumps(doc_obj, ensure_ascii=False) + "\n")

    print("âœ… ë³€í™˜ ì™„ë£Œ: queries.jsonl, docs.jsonl ìƒì„±ë¨")


def split_train_test(
    queries_file,
    docs_file,
    train_query_file,
    test_query_file,
    train_docs_file,
    test_docs_file,
):
    with open(queries_file, "r", encoding="utf-8") as qf:
        queries = [json.loads(line) for line in qf]

    with open(docs_file, "r", encoding="utf-8") as df:
        docs = [json.loads(line) for line in df]
    # ê°œìˆ˜ ì¼ì¹˜ í™•ì¸
    assert len(queries) == len(docs), "âŒ queriesì™€ docs ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤!"
    # 9:1 ë¹„ìœ¨ë¡œ ìˆœì„œ ìœ ì§€ ë¶„í• 
    with open(train_query_file, "w", encoding="utf-8") as tqf, open(
        test_query_file, "w", encoding="utf-8"
    ) as teqf, open(train_docs_file, "w", encoding="utf-8") as tdf, open(
        test_docs_file, "w", encoding="utf-8"
    ) as tedf:

        for i, (q, d) in enumerate(zip(queries, docs)):
            # 9ê°œ train â†’ 1ê°œ test ì£¼ê¸° ë°˜ë³µ
            if i % 10 == 9:  # 10ë²ˆì§¸ë§ˆë‹¤ testë¡œ
                teqf.write(json.dumps(q, ensure_ascii=False) + "\n")
                tedf.write(json.dumps(d, ensure_ascii=False) + "\n")
            else:
                tqf.write(json.dumps(q, ensure_ascii=False) + "\n")
                tdf.write(json.dumps(d, ensure_ascii=False) + "\n")


def split_sessions(
    train_query_file,
    test_query_file,
    train_docs_file,
    test_docs_file,
    base_dir="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/splited",
):
    files = [train_query_file, test_query_file, train_docs_file, test_docs_file]
    for fname in files:
        path = os.path.join(base_dir, fname)
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        total = len(lines)
        chunk_size = math.ceil(total / 10)

        print(f"ğŸ“„ {fname}: {total}ê°œ í•­ëª© â†’ {chunk_size}ê°œì”© 10ë¶„í• ")

        for i in range(10):
            start = i * chunk_size
            end = min((i + 1) * chunk_size, total)
            chunk_lines = lines[start:end]

            out_name = fname.replace(".jsonl", f"_{i}.jsonl")
            out_path = os.path.join(base_dir, out_name)

            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.writelines(chunk_lines)


def _evenly_sample_indices(n: int, k: int):
    """
    ê¸¸ì´ nì—ì„œ kê°œë¥¼ 'ìƒëŒ€ìˆœì„œ ìœ ì§€'í•˜ë©° ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§í•  ì¸ë±ìŠ¤ ëª©ë¡ì„ ìƒì„±.
    - k >= nì´ë©´ 0..n-1 ì „ì²´ ë°˜í™˜
    - k == 1ì´ë©´ [0]
    - ê·¸ ì™¸ì—ëŠ” round(i*(n-1)/(k-1)) ë°©ì‹ìœ¼ë¡œ ë“±ê°„ê²© ì„ íƒ + ì¤‘ë³µ ì œê±°
    """
    if n == 0:
        return []
    if k >= n:
        return list(range(n))
    if k == 1:
        return [0]
    idxs = []
    prev = -1
    for i in range(k):
        # ë“±ê°„ê²© ìœ„ì¹˜ë¥¼ ë°˜ì˜¬ë¦¼ìœ¼ë¡œ ì„ íƒ
        idx = round(i * (n - 1) / (k - 1))
        if idx != prev:
            idxs.append(idx)
            prev = idx
    # í˜¹ì‹œ ì¤‘ë³µ ì œê±° ê³¼ì •ì—ì„œ kê°œë³´ë‹¤ ì‘ì•„ì¡Œë‹¤ë©´ ëì—ì„œ ë³´ì¶©
    j = n - 1
    while len(idxs) < k and j > idxs[-1]:
        idxs.append(j)
        j -= 1
    return idxs[:k]


def sample_query_shards(
    src_base_dir: str,
    dst_base_dir: str,
    train_prefix: str = "train_query",
    test_prefix: str = "test_query",
    train_sampled_prefix: str = "train",
    test_sampled_prefix: str = "test",
    train_per_shard: int = 2430,
    test_per_shard: int = 270,
    num_shards: int = 10,
):
    """
    0~(num_shards-1)ë²ˆ shard íŒŒì¼ì—ì„œ
      - train: ê° shardì—ì„œ train_per_shardê°œ
      - test:  ê° shardì—ì„œ test_per_shardê°œ
    ë¥¼ 'ìƒëŒ€ìˆœì„œ ìœ ì§€'í•˜ë©° ê· ë“± ê°„ê²© ìƒ˜í”Œë§í•˜ê³ ,
    dst_base_dir ì•„ë˜ì— ì €ì¥í•©ë‹ˆë‹¤.

    ì…ë ¥ íŒŒì¼ëª… í˜•ì‹: {train_prefix}_{i}.jsonl, {test_prefix}_{i}.jsonl
    ì¶œë ¥ íŒŒì¼ëª… í˜•ì‹: {train_prefix}_sampled_{i}.jsonl, {test_prefix}_sampled_{i}.jsonl
    """

    os.makedirs(dst_base_dir, exist_ok=True)

    def process(prefix: str, sampled_prefix: str, per_shard: int):
        for i in range(num_shards):
            src_path = os.path.join(src_base_dir, f"{prefix}_{i}.jsonl")
            if not os.path.exists(src_path):
                print(f"âš ï¸  ê±´ë„ˆëœ€: {src_path} (ì—†ìŒ)")
                continue

            with open(src_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            n = len(lines)
            idxs = _evenly_sample_indices(n, per_shard)
            sampled = [lines[idx] for idx in idxs]

            out_path = os.path.join(
                dst_base_dir, f"{sampled_prefix}_session{i}_queries.jsonl"
            )
            with open(out_path, "w", encoding="utf-8") as out:
                out.writelines(sampled)

            print(
                f"âœ… {prefix}_{i}.jsonl â†’ {os.path.basename(out_path)} "
                f"({len(sampled)}/{n}ê°œ, ìˆœì„œ ìœ ì§€ ìƒ˜í”Œë§)"
            )

    # train / test ê°ê° ì²˜ë¦¬
    process(train_prefix, train_sampled_prefix, train_per_shard)
    process(test_prefix, test_sampled_prefix, test_per_shard)


if __name__ == "__main__":
    # input_file = "/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/train.json"
    # queries_file = "/home/work/.default/huijeong/cream/data/news_query.jsonl"
    # docs_file = "/home/work/.default/huijeong/cream/data/news_docs.jsonl"
    # generate_query_and_docs(input_file, queries_file, docs_file)
    # split_train_test(
    #     queries_file = "/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/news_query.jsonl",
    #     docs_file = "/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/news_docs.jsonl",
    #     train_query_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/train_query.jsonl",
    #     test_query_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/test_query.jsonl",
    #     train_docs_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/train_docs.jsonl",
    #     test_docs_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/test_docs.jsonl" )
    # split_sessions(
    #     train_query_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/train_query.jsonl",
    #     test_query_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/test_query.jsonl",
    #     train_docs_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/train_docs.jsonl",
    #     test_docs_file="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/test_docs.jsonl")
    sample_query_shards(
        src_base_dir="/home/work/.default/huijeong/cream/data/ChricinclingAmericaQA/splited",
        dst_base_dir="/home/work/.default/huijeong/cream/data/datasetN_large",
    )
