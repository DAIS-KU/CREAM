nohup: ignoring input
Running experiments: streaming_lsh
Start session: 0
End session: 10
Load cluster: False
Use cluster warming up: stream_seed
Use label: False
Use weight: False
Use tensor key: True
Use nbits: 0
Use max_iters: 3
Use warmingup_rate: None
Use warmingup_k: 5
Use cluster_min_size: 50
Use required_doc_size: 50
Use stream sampling rate: None
Use sampling size per query: 50
Number of Epochs: 1
Include answer: False
RandomProjectionLSH use_tensor_key True
Training Session 0/False
queries:400, docs:11266
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session0_docs.jsonl
queries:400, documents:11266
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 11266 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:11266, document_embeddings:11266
Query-Document encoding ended.(20.960816860198975 sec.)
query_list:400, doc_list:11266
queries #400, documents #11666 initial_docs #0
#doc_stream 12, stream_docs size 402-1024
Session 0 | Document count:11666
############################################Initialize(21.447529554367065sec)############################################
RandomProjectionLSH use_tensor_key True
Initialize centroid
Starting iteration 1 | centroids: #5
cuda:0 | get_closest_clusters batch (0/8)
cuda:0 | get_closest_clusters batch (1/8)
cuda:0 | get_closest_clusters batch (2/8)
cuda:0 | get_closest_clusters batch (3/8)
cuda:0 | get_closest_clusters batch (4/8)
cuda:0 | get_closest_clusters batch (5/8)
cuda:0 | get_closest_clusters batch (6/8)
cuda:0 | get_closest_clusters batch (7/8)
clsuter 0 create new centroid.(instance 263)
clsuter 1 create new centroid.(instance 272)
clsuter 2 create new centroid.(instance 104)
clsuter 3 create new centroid.(instance 143)
clsuter 4 create new centroid.(instance 242)
iter_num 0 | execution_time: 1.4131927490234375 sec.
Starting iteration 2 | centroids: #5
cuda:0 | get_closest_clusters batch (0/8)
cuda:0 | get_closest_clusters batch (1/8)
cuda:0 | get_closest_clusters batch (2/8)
cuda:0 | get_closest_clusters batch (3/8)
cuda:0 | get_closest_clusters batch (4/8)
cuda:0 | get_closest_clusters batch (5/8)
cuda:0 | get_closest_clusters batch (6/8)
cuda:0 | get_closest_clusters batch (7/8)
clsuter 0 create new centroid.(instance 250)
clsuter 1 create new centroid.(instance 241)
clsuter 2 create new centroid.(instance 93)
clsuter 3 create new centroid.(instance 194)
clsuter 4 create new centroid.(instance 246)
iter_num 1 | execution_time: 1.0410211086273193 sec.
Starting iteration 3 | centroids: #5
cuda:0 | get_closest_clusters batch (0/8)
cuda:0 | get_closest_clusters batch (1/8)
cuda:0 | get_closest_clusters batch (2/8)
cuda:0 | get_closest_clusters batch (3/8)
cuda:0 | get_closest_clusters batch (4/8)
cuda:0 | get_closest_clusters batch (5/8)
cuda:0 | get_closest_clusters batch (6/8)
cuda:0 | get_closest_clusters batch (7/8)
clsuter 0 create new centroid.(instance 239)
clsuter 1 create new centroid.(instance 206)
clsuter 2 create new centroid.(instance 90)
clsuter 3 create new centroid.(instance 204)
clsuter 4 create new centroid.(instance 285)
iter_num 2 | execution_time: 1.193014144897461 sec.
cluster 0 size: 239
cluster 1 size: 206
cluster 2 size: 90
cluster 3 size: 204
cluster 4 size: 285
centroids : 5
Create 0th Cluster.
Document only #235/239
Query only #4/239
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([107, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([4, 254, 768]), self.prototype:torch.Size([1, 1, 768])
Create 1th Cluster.
Document only #196/206
Query only #10/206
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([68, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([10, 254, 768]), self.prototype:torch.Size([1, 1, 768])
Create 2th Cluster.
Document only #84/90
Query only #6/90
update_statistics batch_token_embs:torch.Size([84, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([6, 254, 768]), self.prototype:torch.Size([1, 1, 768])
Create 3th Cluster.
Document only #203/204
Query only #1/204
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([75, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([1, 254, 768]), self.prototype:torch.Size([1, 1, 768])
Create 4th Cluster.
Document only #271/285
Query only #14/285
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([15, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([14, 254, 768]), self.prototype:torch.Size([1, 1, 768])
RandomProjectionLSH use_tensor_key True
Spend 3.93990159034729 seconds for clustering(5, 1024) warming up.
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(1.752013921737671sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(1.7714581489562988sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(1.9147765636444092sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(1.855872631072998sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(1.9362549781799316sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(1.8999626636505127sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.0914928913116455sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(1.9331107139587402sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(1.9545090198516846sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.1560280323028564sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 18 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(0.8017697334289551sec).
############################################Assign(20.067249298095703sec)############################################
Document only #2727/2753
Query only #26/2753
Document only #1886/1938
Query only #52/1938
Document only #936/979
Query only #43/979
Document only #2296/2330
Query only #34/2330
Document only #3421/3666
Query only #245/3666
Session 0 | SSE: 572342152.875
Document only #2727/2753
Document only #1886/1938
Document only #936/979
Document only #2296/2330
Document only #3421/3666
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #26/2753
Document only #2727/2753
BOUNDARY: 220.4141741811788| mean:216.30615280776988, std:16.43208549363568, z1:8.0, z2:0.25
* Evict result docs 1294, queries 12, total 1306
Document only #1294/1306
Query only #12/1306
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([14, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([12, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 2727 -> 1294,queries# 26 -> 12, new std:14.996707340313822
Query only #52/1938
Document only #1886/1938
BOUNDARY: 222.40037840606684| mean:218.0858327264141, std:17.25818271861087, z1:8.0, z2:0.25
* Evict result docs 982, queries 27, total 1009
Document only #982/1009
Query only #27/1009
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([86, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([27, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1886 -> 982,queries# 52 -> 27, new std:16.54483481223898
Query only #43/979
Document only #936/979
BOUNDARY: 224.96829043682823| mean:220.9596990384656, std:16.03436559345045, z1:8.0, z2:0.25
* Evict result docs 580, queries 26, total 606
Document only #580/606
Query only #26/606
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([68, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([26, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 936 -> 580,queries# 43 -> 26, new std:15.534904237699893
Query only #34/2330
Document only #2296/2330
BOUNDARY: 226.16623969249147| mean:222.92774599181735, std:12.953974802696482, z1:8.0, z2:0.25
* Evict result docs 1244, queries 18, total 1262
Document only #1244/1262
Query only #18/1262
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([92, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([18, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 2296 -> 1244,queries# 34 -> 18, new std:12.33336686864955
Query only #245/3666
Document only #3421/3666
BOUNDARY: 228.0935576172405| mean:224.75298327240083, std:13.362297379358642, z1:8.0, z2:0.25
* Evict result docs 2339, queries 167, total 2506
Document only #2339/2506
Query only #167/2506
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([35, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([39, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3421 -> 2339,queries# 245 -> 167, new std:13.162848797281406
clear_unused_documents | before total #11666
clear_unused_documents | after total #6689
############################################Eviction(20.88451600074768sec)############################################
Training Session 1/False
queries:400, docs:12062
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session1_docs.jsonl
queries:400, documents:12062
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12062 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:12062, document_embeddings:12062
Query-Document encoding ended.(22.233526945114136 sec.)
query_list:400, doc_list:12062
queries #400, documents #19151 initial_docs #0
#doc_stream 13, stream_docs size 174-1024
Session 1 | Document count:19151
############################################Initialize(22.569814682006836sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 1625
1th size: 1198
2th size: 695
3th size: 1435
4th size: 2760
Assign 0th stream ended(1.984555959701538sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(1.9696133136749268sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(1.9194419384002686sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.124553918838501sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.074493408203125sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.260051727294922sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(2.0988519191741943sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.294140577316284sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(2.306417465209961sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(2.301862955093384sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.3163838386535645sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(2.2526907920837402sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 46 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.3737070560455322sec).
############################################Assign(26.27676486968994sec)############################################
Document only #4998/5092
Query only #94/5092
Document only #3264/3382
Query only #118/3382
Document only #1486/1554
Query only #68/1554
Document only #3405/3458
Query only #53/3458
Document only #5348/5665
Query only #317/5665
Session 1 | SSE: 918491658.84375
Document only #4998/5092
Document only #3264/3382
Document only #1486/1554
Document only #3405/3458
Document only #5348/5665
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #94/5092
Document only #4998/5092
BOUNDARY: 219.9832229454318| mean:215.7424319390228, std:16.963164025635994, z1:8.0, z2:0.25
* Evict result docs 2656, queries 49, total 2705
Document only #2656/2705
Query only #49/2705
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([96, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([49, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 4998 -> 2656,queries# 94 -> 49, new std:14.845159542155951
Query only #118/3382
Document only #3264/3382
BOUNDARY: 219.68267637994987| mean:215.18026788702807, std:18.009633971687233, z1:8.0, z2:0.25
* Evict result docs 1787, queries 64, total 1851
Document only #1787/1851
Query only #64/1851
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([123, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([64, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3264 -> 1787,queries# 118 -> 64, new std:16.882154984913726
Query only #68/1554
Document only #1486/1554
BOUNDARY: 221.7338718263759| mean:217.65797824388613, std:16.303574329958977, z1:8.0, z2:0.25
* Evict result docs 923, queries 42, total 965
Document only #923/965
Query only #42/965
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([27, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([42, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1486 -> 923,queries# 68 -> 42, new std:15.965887588755907
Query only #53/3458
Document only #3405/3458
BOUNDARY: 222.9949439758864| mean:219.55445823602554, std:13.761942959443441, z1:8.0, z2:0.25
* Evict result docs 1799, queries 28, total 1827
Document only #1799/1827
Query only #28/1827
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([7, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([28, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3405 -> 1799,queries# 53 -> 28, new std:12.713925550256446
Query only #317/5665
Document only #5348/5665
BOUNDARY: 225.626040086146| mean:222.328187128115, std:13.19141183212405, z1:8.0, z2:0.25
* Evict result docs 3669, queries 217, total 3886
Document only #3669/3886
Query only #217/3886
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([85, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([89, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 5348 -> 3669,queries# 317 -> 217, new std:12.993833360620213
clear_unused_documents | before total #19151
clear_unused_documents | after total #11234
############################################Eviction(33.65552234649658sec)############################################
Training Session 2/False
queries:400, docs:12625
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session2_docs.jsonl
queries:400, documents:12625
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12625 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12625, document_embeddings:12625
Query-Document encoding ended.(24.12255883216858 sec.)
query_list:400, doc_list:12625
queries #400, documents #24259 initial_docs #0
#doc_stream 13, stream_docs size 737-1024
Session 2 | Document count:24259
############################################Initialize(24.44342017173767sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 3402
1th size: 1967
2th size: 975
3th size: 1871
4th size: 4043
Assign 0th stream ended(2.117342948913574sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.35587739944458sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.2745754718780518sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.405597686767578sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.552022933959961sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.498995542526245sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(2.6377170085906982sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.5861966609954834sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(2.8501172065734863sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(2.776801586151123sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.873250961303711sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(2.868363380432129sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 97 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(2.211637258529663sec).
############################################Assign(33.008496046066284sec)############################################
Document only #10460/10721
Query only #261/10721
Document only #3800/3961
Query only #161/3961
Document only #1049/1095
Query only #46/1095
Document only #2434/2463
Query only #29/2463
Document only #5716/6019
Query only #303/6019
Session 2 | SSE: 1126958589.8046875
Document only #10460/10721
Document only #3800/3961
Document only #1049/1095
Document only #2434/2463
Document only #5716/6019
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #261/10721
Document only #10460/10721
BOUNDARY: 218.15930013305686| mean:213.89806482936672, std:17.04494121476062, z1:8.0, z2:0.25
* Evict result docs 5452, queries 136, total 5588
Document only #5452/5588
Query only #136/5588
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([76, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([8, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 10460 -> 5452,queries# 261 -> 136, new std:15.277804359938385
Query only #161/3961
Document only #3800/3961
BOUNDARY: 216.1748028208876| mean:211.45745366423517, std:18.869396626609728, z1:8.0, z2:0.25
* Evict result docs 2185, queries 92, total 2277
Document only #2185/2277
Query only #92/2277
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([9, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([92, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3800 -> 2185,queries# 161 -> 92, new std:17.222595267365122
Query only #46/1095
Document only #1049/1095
BOUNDARY: 215.53929893082505| mean:211.5023596907315, std:16.147756960374174, z1:8.0, z2:0.25
* Evict result docs 559, queries 24, total 583
Document only #559/583
Query only #24/583
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([47, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([24, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1049 -> 559,queries# 46 -> 24, new std:16.509499572389895
Query only #29/2463
Document only #2434/2463
BOUNDARY: 216.83200361844354| mean:213.39250109576133, std:13.758010090728861, z1:8.0, z2:0.25
* Evict result docs 1276, queries 15, total 1291
Document only #1276/1291
Query only #15/1291
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([124, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([15, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 2434 -> 1276,queries# 29 -> 15, new std:11.578811630112682
Query only #303/6019
Document only #5716/6019
BOUNDARY: 223.50404205008348| mean:220.2006340609335, std:13.213631956599858, z1:8.0, z2:0.25
* Evict result docs 3835, queries 203, total 4038
Document only #3835/4038
Query only #203/4038
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([123, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([75, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 5716 -> 3835,queries# 303 -> 203, new std:13.1164307785386
clear_unused_documents | before total #24259
clear_unused_documents | after total #13777
############################################Eviction(45.49558734893799sec)############################################
Training Session 3/False
queries:400, docs:13375
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session3_docs.jsonl
queries:400, documents:13375
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 13375 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:13375, document_embeddings:13375
Query-Document encoding ended.(24.98589062690735 sec.)
query_list:400, doc_list:13375
queries #400, documents #27552 initial_docs #0
#doc_stream 14, stream_docs size 463-1024
Session 3 | Document count:27552
############################################Initialize(25.365944385528564sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 6306
1th size: 2369
2th size: 587
3th size: 1322
4th size: 4217
Assign 0th stream ended(2.5751447677612305sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.681948661804199sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.6321628093719482sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.8304619789123535sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.990286350250244sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(3.0966858863830566sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.1953015327453613sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(3.0100722312927246sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.1733176708221436sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.2855522632598877sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.4380695819854736sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.497615337371826sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(3.5226099491119385sec).
Assign 13th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 79 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 13th stream ended(1.6036262512207031sec).
############################################Assign(41.53285527229309sec)############################################
Document only #14473/14863
Query only #390/14863
Document only #3455/3558
Query only #103/3558
Document only #623/648
Query only #25/648
Document only #1747/1766
Query only #19/1766
Document only #6384/6717
Query only #333/6717
Session 3 | SSE: 1236299068.46875
Document only #14473/14863
Document only #3455/3558
Document only #623/648
Document only #1747/1766
Document only #6384/6717
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #390/14863
Document only #14473/14863
BOUNDARY: 214.7406786882405| mean:210.3190373702589, std:17.686565271926394, z1:8.0, z2:0.25
* Evict result docs 7884, queries 212, total 8096
Document only #7884/8096
Query only #212/8096
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([76, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([84, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 14473 -> 7884,queries# 390 -> 212, new std:15.197441302681383
Query only #103/3558
Document only #3455/3558
BOUNDARY: 208.8553889895217| mean:204.23967987027578, std:18.46283647698375, z1:8.0, z2:0.25
* Evict result docs 2111, queries 62, total 2173
Document only #2111/2173
Query only #62/2173
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([63, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([62, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3455 -> 2111,queries# 103 -> 62, new std:15.809000175009873
Query only #25/648
Document only #623/648
BOUNDARY: 208.43542326800753| mean:204.0952024463518, std:17.360883286622926, z1:8.0, z2:0.25
* Evict result docs 378, queries 15, total 393
Document only #378/393
Query only #15/393
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([122, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([15, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 623 -> 378,queries# 25 -> 15, new std:16.246342134665714
Query only #19/1766
Document only #1747/1766
BOUNDARY: 211.34249011309203| mean:207.62145114201516, std:14.884155884307493, z1:8.0, z2:0.25
* Evict result docs 1088, queries 11, total 1099
Document only #1088/1099
Query only #11/1099
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([64, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([11, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1747 -> 1088,queries# 19 -> 11, new std:11.197894884317813
Query only #333/6717
Document only #6384/6717
BOUNDARY: 221.57685111071075| mean:218.12663197670022, std:13.800876536042043, z1:8.0, z2:0.25
* Evict result docs 4182, queries 218, total 4400
Document only #4182/4400
Query only #218/4400
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([86, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([90, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 6384 -> 4182,queries# 333 -> 218, new std:13.744714992342988
clear_unused_documents | before total #27552
clear_unused_documents | after total #16161
############################################Eviction(53.374340772628784sec)############################################
Training Session 4/False
queries:400, docs:12179
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session4_docs.jsonl
queries:400, documents:12179
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12179 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:12179, document_embeddings:12179
Query-Document encoding ended.(22.050466299057007 sec.)
query_list:400, doc_list:12179
queries #400, documents #28740 initial_docs #0
#doc_stream 13, stream_docs size 291-1024
Session 4 | Document count:28740
############################################Initialize(22.37320613861084sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 8494
1th size: 2251
2th size: 401
3th size: 1222
4th size: 4817
Assign 0th stream ended(2.670891046524048sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.4923272132873535sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.7901363372802734sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.8468236923217773sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.90891432762146sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.8435094356536865sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.0614559650421143sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.8442373275756836sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(2.965390205383301sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.0892319679260254sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.019906520843506sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.0618057250976562sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 35 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.945758581161499sec).
############################################Assign(35.540388345718384sec)############################################
Document only #12235/12528
Query only #293/12528
Document only #3252/3321
Query only #69/3321
Document only #459/475
Query only #16/475
Document only #2706/2726
Query only #20/2726
Document only #9170/9690
Query only #520/9690
Session 4 | SSE: 1261927075.96875
Document only #12235/12528
Document only #3252/3321
Document only #459/475
Document only #2706/2726
Document only #9170/9690
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #293/12528
Document only #12235/12528
BOUNDARY: 208.3775123047977| mean:203.95878402566203, std:17.674913116542704, z1:8.0, z2:0.25
* Evict result docs 7640, queries 182, total 7822
Document only #7640/7822
Query only #182/7822
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([88, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([54, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 12235 -> 7640,queries# 293 -> 182, new std:14.25024717681241
Query only #69/3321
Document only #3252/3321
BOUNDARY: 203.6500156196535| mean:199.06066261842574, std:18.35741200491105, z1:8.0, z2:0.25
* Evict result docs 2159, queries 45, total 2204
Document only #2159/2204
Query only #45/2204
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([111, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([45, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3252 -> 2159,queries# 69 -> 45, new std:14.640180204594088
Query only #16/475
Document only #459/475
BOUNDARY: 204.749140883238| mean:200.12725285178738, std:18.487552125802544, z1:8.0, z2:0.25
* Evict result docs 319, queries 11, total 330
Document only #319/330
Query only #11/330
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([63, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([11, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 459 -> 319,queries# 16 -> 11, new std:15.766834951025471
Query only #20/2726
Document only #2706/2726
BOUNDARY: 215.95330668830664| mean:211.69365884334132, std:17.038591379861256, z1:8.0, z2:0.25
* Evict result docs 1564, queries 11, total 1575
Document only #1564/1575
Query only #11/1575
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([28, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([11, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 2706 -> 1564,queries# 20 -> 11, new std:11.209252333405374
Query only #520/9690
Document only #9170/9690
BOUNDARY: 221.62491870217633| mean:217.845544025182, std:15.117498707977294, z1:8.0, z2:0.25
* Evict result docs 6354, queries 360, total 6714
Document only #6354/6714
Query only #360/6714
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([82, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([104, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 9170 -> 6354,queries# 520 -> 360, new std:14.654820862865394
clear_unused_documents | before total #28740
clear_unused_documents | after total #18645
############################################Eviction(60.6951699256897sec)############################################
Training Session 5/False
queries:400, docs:11993
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session5_docs.jsonl
queries:400, documents:11993
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 11993 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:11993, document_embeddings:11993
Query-Document encoding ended.(20.79843020439148 sec.)
query_list:400, doc_list:11993
queries #400, documents #31038 initial_docs #0
#doc_stream 13, stream_docs size 105-1024
Session 5 | Document count:31038
############################################Initialize(21.12226128578186sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 7899
1th size: 2300
2th size: 405
3th size: 1878
4th size: 7187
Assign 0th stream ended(2.5392050743103027sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.6333117485046387sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.538227081298828sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.6382200717926025sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.7681539058685303sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.8347554206848145sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(2.80599045753479sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.7273361682891846sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.075439691543579sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.1067311763763428sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.080528974533081sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.015566349029541sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 105 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.43030285835266113sec).
############################################Assign(34.1937689781189sec)############################################
Document only #8744/8928
Query only #184/8928
Document only #3374/3425
Query only #51/3425
Document only #1123/1184
Query only #61/1184
Document only #4902/4960
Query only #58/4960
Document only #11886/12541
Query only #655/12541
Session 5 | SSE: 1363232819.375
Document only #8744/8928
Document only #3374/3425
Document only #1123/1184
Document only #4902/4960
Document only #11886/12541
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #184/8928
Document only #8744/8928
BOUNDARY: 200.659214564872| mean:196.68508595919843, std:15.896514422694226, z1:8.0, z2:0.25
* Evict result docs 5762, queries 121, total 5883
Document only #5762/5883
Query only #121/5883
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([2, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([121, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 8744 -> 5762,queries# 184 -> 121, new std:13.44675800655511
Query only #51/3425
Document only #3374/3425
BOUNDARY: 201.85214200727512| mean:197.15864824302005, std:18.773975057020216, z1:8.0, z2:0.25
* Evict result docs 2379, queries 35, total 2414
Document only #2379/2414
Query only #35/2414
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([75, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([35, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3374 -> 2379,queries# 51 -> 35, new std:13.64320979188758
Query only #61/1184
Document only #1123/1184
BOUNDARY: 218.49104513966307| mean:213.39620190395698, std:20.37937294282435, z1:8.0, z2:0.25
* Evict result docs 623, queries 33, total 656
Document only #623/656
Query only #33/656
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([111, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([33, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1123 -> 623,queries# 61 -> 33, new std:18.24523780538146
Query only #58/4960
Document only #4902/4960
BOUNDARY: 219.55934820555393| mean:215.34656771556024, std:16.851121959974765, z1:8.0, z2:0.25
* Evict result docs 2527, queries 29, total 2556
Document only #2527/2556
Query only #29/2556
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([95, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([29, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 4902 -> 2527,queries# 58 -> 29, new std:12.44476358859575
Query only #655/12541
Document only #11886/12541
BOUNDARY: 221.18304569456964| mean:217.37807330232746, std:15.219889568968707, z1:8.0, z2:0.25
* Evict result docs 8254, queries 454, total 8708
Document only #8254/8708
Query only #454/8708
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([62, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([70, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 11886 -> 8254,queries# 655 -> 454, new std:14.615108028115966
clear_unused_documents | before total #31038
clear_unused_documents | after total #20217
############################################Eviction(63.721474170684814sec)############################################
Training Session 6/False
queries:400, docs:12039
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session6_docs.jsonl
queries:400, documents:12039
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12039 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:12039, document_embeddings:12039
Query-Document encoding ended.(20.692071199417114 sec.)
query_list:400, doc_list:12039
queries #400, documents #32656 initial_docs #0
#doc_stream 13, stream_docs size 151-1024
Session 6 | Document count:32656
############################################Initialize(21.070329427719116sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 5990
1th size: 2622
2th size: 728
3th size: 2787
4th size: 9114
Assign 0th stream ended(2.748185634613037sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.4259464740753174sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.733261823654175sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.940497636795044sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.8338265419006348sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.8438260555267334sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.044370651245117sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(3.0212724208831787sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.0158135890960693sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.0459742546081543sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.0570926666259766sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.111743211746216sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 23 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.4483363628387451sec).
############################################Assign(35.2701473236084sec)############################################
Document only #6907/7046
Query only #139/7046
Document only #4829/4939
Query only #110/4939
Document only #1486/1557
Query only #71/1557
Document only #5449/5541
Query only #92/5541
Document only #12913/13573
Query only #660/13573
Session 6 | SSE: 1435214276.8125
Document only #6907/7046
Document only #4829/4939
Document only #1486/1557
Document only #5449/5541
Document only #12913/13573
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #139/7046
Document only #6907/7046
BOUNDARY: 198.25401684526952| mean:193.98261508050373, std:17.08560705906315, z1:8.0, z2:0.25
* Evict result docs 5226, queries 105, total 5331
Document only #5226/5331
Query only #105/5331
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([106, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([105, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 6907 -> 5226,queries# 139 -> 105, new std:13.277785671103288
Query only #110/4939
Document only #4829/4939
BOUNDARY: 207.914820239692| mean:202.6497231354282, std:21.060388417055314, z1:8.0, z2:0.25
* Evict result docs 3111, queries 70, total 3181
Document only #3111/3181
Query only #70/3181
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([39, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([70, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 4829 -> 3111,queries# 110 -> 70, new std:14.821540887547444
Query only #71/1557
Document only #1486/1557
BOUNDARY: 217.49774638542746| mean:212.5244141146397, std:19.893329083151045, z1:8.0, z2:0.25
* Evict result docs 879, queries 41, total 920
Document only #879/920
Query only #41/920
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([111, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([41, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1486 -> 879,queries# 71 -> 41, new std:17.7979017884756
Query only #92/5541
Document only #5449/5541
BOUNDARY: 217.92763712888535| mean:213.7377391124413, std:16.75959206577626, z1:8.0, z2:0.25
* Evict result docs 3124, queries 52, total 3176
Document only #3124/3176
Query only #52/3176
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([52, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([52, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 5449 -> 3124,queries# 92 -> 52, new std:12.651104858408354
Query only #660/13573
Document only #12913/13573
BOUNDARY: 219.9879808962824| mean:216.18742827791934, std:15.202210473452263, z1:8.0, z2:0.25
* Evict result docs 8615, queries 440, total 9055
Document only #8615/9055
Query only #440/9055
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([39, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([56, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 12913 -> 8615,queries# 660 -> 440, new std:14.579079323246312
clear_unused_documents | before total #32656
clear_unused_documents | after total #21663
############################################Eviction(68.40374326705933sec)############################################
Training Session 7/False
queries:400, docs:12414
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session7_docs.jsonl
queries:400, documents:12414
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12414 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12414, document_embeddings:12414
Query-Document encoding ended.(21.879171133041382 sec.)
query_list:400, doc_list:12414
queries #400, documents #34477 initial_docs #0
#doc_stream 13, stream_docs size 526-1024
Session 7 | Document count:34477
############################################Initialize(22.204550743103027sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 5824
1th size: 3340
2th size: 930
3th size: 3246
4th size: 9347
Assign 0th stream ended(2.9282569885253906sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.906428575515747sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.7741572856903076sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.8713908195495605sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.8709335327148438sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.9761860370635986sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.0499138832092285sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.9899044036865234sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.2074637413024902sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.060173273086548sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.2143096923828125sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.149942636489868sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 14 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(1.8115167617797852sec).
############################################Assign(37.810577630996704sec)############################################
Document only #11081/11354
Query only #273/11354
Document only #5252/5393
Query only #141/5393
Document only #1027/1068
Query only #41/1068
Document only #4092/4149
Query only #57/4149
Document only #11917/12513
Query only #596/12513
Session 7 | SSE: 1490489681.1875
Document only #11081/11354
Document only #5252/5393
Document only #1027/1068
Document only #4092/4149
Document only #11917/12513
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #273/11354
Document only #11081/11354
BOUNDARY: 207.7321212038541| mean:202.56206987624188, std:20.680205310448905, z1:8.0, z2:0.25
* Evict result docs 6855, queries 168, total 7023
Document only #6855/7023
Query only #168/7023
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([71, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([40, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 11081 -> 6855,queries# 273 -> 168, new std:14.291918894237126
Query only #141/5393
Document only #5252/5393
BOUNDARY: 206.26415698465559| mean:201.0707787598592, std:20.773512899185583, z1:8.0, z2:0.25
* Evict result docs 3640, queries 97, total 3737
Document only #3640/3737
Query only #97/3737
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([56, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([97, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 5252 -> 3640,queries# 141 -> 97, new std:15.337216627819132
Query only #41/1068
Document only #1027/1068
BOUNDARY: 208.28960093684174| mean:203.6802508009507, std:18.437400543564138, z1:8.0, z2:0.25
* Evict result docs 636, queries 25, total 661
Document only #636/661
Query only #25/661
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([124, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([25, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 1027 -> 636,queries# 41 -> 25, new std:16.444888696174836
Query only #57/4149
Document only #4092/4149
BOUNDARY: 211.22053647690777| mean:207.32204804658372, std:15.593953721296174, z1:8.0, z2:0.25
* Evict result docs 2699, queries 37, total 2736
Document only #2699/2736
Query only #37/2736
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([11, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([37, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 4092 -> 2699,queries# 57 -> 37, new std:11.775511160702726
Query only #596/12513
Document only #11917/12513
BOUNDARY: 217.75953059806926| mean:213.9131779987578, std:15.385410397245867, z1:8.0, z2:0.25
* Evict result docs 7655, queries 382, total 8037
Document only #7655/8037
Query only #382/8037
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([103, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([126, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 11917 -> 7655,queries# 596 -> 382, new std:14.740110198015525
clear_unused_documents | before total #34477
clear_unused_documents | after total #22194
############################################Eviction(72.79668831825256sec)############################################
Training Session 8/False
queries:400, docs:12789
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session8_docs.jsonl
queries:400, documents:12789
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12789 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12789, document_embeddings:12789
Query-Document encoding ended.(22.445236682891846 sec.)
query_list:400, doc_list:12789
queries #400, documents #35383 initial_docs #0
#doc_stream 13, stream_docs size 901-1024
Session 8 | Document count:35383
############################################Initialize(22.76636552810669sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 7625
1th size: 3819
2th size: 664
3th size: 2770
4th size: 8340
Assign 0th stream ended(2.8603827953338623sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(3.052793025970459sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.963364362716675sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(3.0984480381011963sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(3.0759575366973877sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(3.2116470336914062sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.39977765083313sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(3.2809767723083496sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.721496820449829sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.541607618331909sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.597851037979126sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.885655403137207sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 5 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(3.2568888664245605sec).
############################################Assign(42.9468469619751sec)############################################
Document only #14491/14859
Query only #368/14859
Document only #4713/4814
Query only #101/4814
Document only #688/714
Query only #26/714
Document only #3207/3245
Query only #38/3245
Document only #11175/11751
Query only #576/11751
Session 8 | SSE: 1493286390.34375
Document only #14491/14859
Document only #4713/4814
Document only #688/714
Document only #3207/3245
Document only #11175/11751
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #368/14859
Document only #14491/14859
BOUNDARY: 208.05322327527196| mean:202.9453331785557, std:20.431560386865062, z1:8.0, z2:0.25
* Evict result docs 9129, queries 231, total 9360
Document only #9129/9360
Query only #231/9360
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([41, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([103, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 14491 -> 9129,queries# 368 -> 231, new std:14.614632821739182
Query only #101/4814
Document only #4713/4814
BOUNDARY: 198.84604423153985| mean:194.40960955357323, std:17.745738711866476, z1:8.0, z2:0.25
* Evict result docs 3324, queries 71, total 3395
Document only #3324/3395
Query only #71/3395
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([124, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([71, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 4713 -> 3324,queries# 101 -> 71, new std:14.439301491126312
Query only #26/714
Document only #688/714
BOUNDARY: 200.80794622463992| mean:196.39347139793952, std:17.657899306801646, z1:8.0, z2:0.25
* Evict result docs 467, queries 17, total 484
Document only #467/484
Query only #17/484
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([83, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([17, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 688 -> 467,queries# 26 -> 17, new std:16.140713026841553
Query only #38/3245
Document only #3207/3245
BOUNDARY: 206.06287449603383| mean:202.48772545654344, std:14.300596157961554, z1:8.0, z2:0.25
* Evict result docs 2076, queries 24, total 2100
Document only #2076/2100
Query only #24/2100
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([28, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([24, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3207 -> 2076,queries# 38 -> 24, new std:11.161624368670653
Query only #576/11751
Document only #11175/11751
BOUNDARY: 215.82159999353743| mean:211.8252051921715, std:15.985579205463722, z1:8.0, z2:0.25
* Evict result docs 7360, queries 379, total 7739
Document only #7360/7739
Query only #379/7739
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([64, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([123, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 11175 -> 7360,queries# 576 -> 379, new std:15.05410503897076
clear_unused_documents | before total #35383
clear_unused_documents | after total #23078
############################################Eviction(75.85117626190186sec)############################################
Training Session 9/False
queries:400, docs:12532
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session9_docs.jsonl
queries:400, documents:12532
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12532 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12532, document_embeddings:12532
Query-Document encoding ended.(21.74763798713684 sec.)
query_list:400, doc_list:12532
queries #400, documents #36010 initial_docs #0
#doc_stream 13, stream_docs size 644-1024
Session 9 | Document count:36010
############################################Initialize(22.07186269760132sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 9675
1th size: 3484
2th size: 490
3th size: 2237
4th size: 8216
Assign 0th stream ended(2.9843804836273193sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(3.1197259426116943sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(3.0180113315582275sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(3.117947578430176sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(3.083967924118042sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(3.254619836807251sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.2144699096679688sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(3.4052462577819824sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.239797830581665sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.597370147705078sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.4059369564056396sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.529123067855835sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 4 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(2.457818031311035sec).
############################################Assign(41.428415298461914sec)############################################
Document only #13072/13364
Query only #292/13364
Document only #4396/4474
Query only #78/4474
Document only #558/578
Query only #20/578
Document only #3855/3891
Query only #36/3891
Document only #13007/13703
Query only #696/13703
Session 9 | SSE: 1501657194.6875
Document only #13072/13364
Document only #4396/4474
Document only #558/578
Document only #3855/3891
Document only #13007/13703
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #292/13364
Document only #13072/13364
BOUNDARY: 202.10685346765757| mean:197.42392166854913, std:18.731727196433834, z1:8.0, z2:0.25
* Evict result docs 9414, queries 210, total 9624
Document only #9414/9624
Query only #210/9624
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([70, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([82, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 13072 -> 9414,queries# 292 -> 210, new std:13.801340422584204
Query only #78/4474
Document only #4396/4474
BOUNDARY: 195.75610095458032| mean:191.41776873706226, std:17.3533288700722, z1:8.0, z2:0.25
* Evict result docs 3133, queries 55, total 3188
Document only #3133/3188
Query only #55/3188
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([61, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([55, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 4396 -> 3133,queries# 78 -> 55, new std:13.713221329171079
Query only #20/578
Document only #558/578
BOUNDARY: 199.4220445804546| mean:194.59720446040473, std:19.29936048019958, z1:8.0, z2:0.25
* Evict result docs 453, queries 16, total 469
Document only #453/469
Query only #16/469
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([69, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([16, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 558 -> 453,queries# 20 -> 16, new std:16.069598161985745
Query only #36/3891
Document only #3855/3891
BOUNDARY: 211.3265799836466| mean:206.81399531331965, std:18.050338681307835, z1:8.0, z2:0.25
* Evict result docs 2515, queries 23, total 2538
Document only #2515/2538
Query only #23/2538
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([83, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([23, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 3855 -> 2515,queries# 36 -> 23, new std:10.999597486408353
Query only #696/13703
Document only #13007/13703
BOUNDARY: 216.47367903387513| mean:212.21997069535635, std:17.014833354075062, z1:8.0, z2:0.25
* Evict result docs 8981, queries 480, total 9461
Document only #8981/9461
Query only #480/9461
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([21, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 1, 768])
update_statistics batch_token_embs:torch.Size([96, 254, 768]), self.prototype:torch.Size([1, 1, 768])
doc_ids# 13007 -> 8981,queries# 696 -> 480, new std:15.312981786353175
clear_unused_documents | before total #36010
clear_unused_documents | after total #25280
############################################Eviction(82.76845598220825sec)############################################
