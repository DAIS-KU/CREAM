nohup: ignoring input
Running experiments: streaming_lsh
Start session: 0
End session: 10
Load cluster: False
Use cluster warming up: stream_seed
Use label: False
Use weight: False
Use tensor key: True
Use nbits: 3
Use max_iters: 3
Use warmingup_rate: None
Use warmingup_k: 5
Use cluster_min_size: 50
Use required_doc_size: 50
Use stream sampling rate: None
Use sampling size per query: 50
Number of Epochs: 1
Include answer: False
RandomProjectionLSH use_tensor_key True
Training Session 0/False
queries:400, docs:11266
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session0_docs.jsonl
queries:400, documents:11266
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 11266 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:11266, document_embeddings:11266
Query-Document encoding ended.(20.925609588623047 sec.)
query_list:400, doc_list:11266
queries #400, documents #11666 initial_docs #0
#doc_stream 12, stream_docs size 402-1024
Session 0 | Document count:11666
############################################Initialize(21.552256107330322sec)############################################
RandomProjectionLSH use_tensor_key True
Initialize centroid
Starting iteration 1 | centroids: #5
cuda:0 | get_closest_clusters batch (0/8)
cuda:0 | get_closest_clusters batch (1/8)
cuda:0 | get_closest_clusters batch (2/8)
cuda:0 | get_closest_clusters batch (3/8)
cuda:0 | get_closest_clusters batch (4/8)
cuda:0 | get_closest_clusters batch (5/8)
cuda:0 | get_closest_clusters batch (6/8)
cuda:0 | get_closest_clusters batch (7/8)
clsuter 0 create new centroid.(instance 76)
clsuter 1 create new centroid.(instance 309)
clsuter 2 create new centroid.(instance 3)
clsuter 3 create new centroid.(instance 251)
clsuter 4 create new centroid.(instance 385)
iter_num 0 | execution_time: 1.5697600841522217 sec.
Starting iteration 2 | centroids: #5
cuda:0 | get_closest_clusters batch (0/8)
cuda:0 | get_closest_clusters batch (1/8)
cuda:0 | get_closest_clusters batch (2/8)
cuda:0 | get_closest_clusters batch (3/8)
cuda:0 | get_closest_clusters batch (4/8)
cuda:0 | get_closest_clusters batch (5/8)
cuda:0 | get_closest_clusters batch (6/8)
cuda:0 | get_closest_clusters batch (7/8)
clsuter 0 create new centroid.(instance 169)
clsuter 1 create new centroid.(instance 298)
clsuter 2 create new centroid.(instance 16)
clsuter 3 create new centroid.(instance 208)
clsuter 4 create new centroid.(instance 333)
iter_num 1 | execution_time: 1.1635663509368896 sec.
Starting iteration 3 | centroids: #5
cuda:0 | get_closest_clusters batch (0/8)
cuda:0 | get_closest_clusters batch (1/8)
cuda:0 | get_closest_clusters batch (2/8)
cuda:0 | get_closest_clusters batch (3/8)
cuda:0 | get_closest_clusters batch (4/8)
cuda:0 | get_closest_clusters batch (5/8)
cuda:0 | get_closest_clusters batch (6/8)
cuda:0 | get_closest_clusters batch (7/8)
clsuter 0 create new centroid.(instance 211)
clsuter 1 create new centroid.(instance 305)
clsuter 2 create new centroid.(instance 85)
clsuter 3 create new centroid.(instance 169)
clsuter 4 create new centroid.(instance 254)
iter_num 2 | execution_time: 1.371248483657837 sec.
cluster 0 size: 211
cluster 1 size: 305
cluster 2 size: 85
cluster 3 size: 169
cluster 4 size: 254
centroids : 5
Create 0th Cluster.
Document only #211/211
Query only #0/211
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([83, 254, 768]), self.prototype:torch.Size([1, 8, 768])
Create 1th Cluster.
Document only #305/305
Query only #0/305
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([49, 254, 768]), self.prototype:torch.Size([1, 8, 768])
Create 2th Cluster.
Document only #43/85
Query only #42/85
update_statistics batch_token_embs:torch.Size([43, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([42, 254, 768]), self.prototype:torch.Size([1, 8, 768])
Create 3th Cluster.
Document only #168/169
Query only #1/169
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([40, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([1, 254, 768]), self.prototype:torch.Size([1, 8, 768])
Create 4th Cluster.
Document only #254/254
Query only #0/254
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([126, 254, 768]), self.prototype:torch.Size([1, 8, 768])
RandomProjectionLSH use_tensor_key True
Spend 4.366899490356445 seconds for clustering(5, 1024) warming up.
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(1.860896110534668sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(1.8064806461334229sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(1.9373528957366943sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(1.8418922424316406sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(1.9875640869140625sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(1.9251322746276855sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(1.9448864459991455sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(1.999147653579712sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(1.9986672401428223sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.048295021057129sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 18 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(0.8227658271789551sec).
############################################Assign(20.173080444335938sec)############################################
Document only #3094/3220
Query only #126/3220
Document only #3271/3342
Query only #71/3342
Document only #1197/1287
Query only #90/1287
Document only #1701/1760
Query only #59/1760
Document only #2003/2057
Query only #54/2057
Session 0 | SSE: 562225818.375
Document only #3094/3220
Document only #3271/3342
Document only #1197/1287
Document only #1701/1760
Document only #2003/2057
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #126/3220
Document only #3094/3220
BOUNDARY: 224.9033610912873| mean:221.29017122043587, std:14.452759483405723, z1:8.0, z2:0.25
* Evict result docs 1924, queries 78, total 2002
Document only #1924/2002
Query only #78/2002
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([4, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([78, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 3094 -> 1924,queries# 126 -> 78, new std:14.085901424194256
Query only #71/3342
Document only #3271/3342
BOUNDARY: 223.94573269496664| mean:220.34255064125907, std:14.412728214830308, z1:8.0, z2:0.25
* Evict result docs 1806, queries 39, total 1845
Document only #1806/1845
Query only #39/1845
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([14, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([39, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 3271 -> 1806,queries# 71 -> 39, new std:14.259045191017778
Query only #90/1287
Document only #1197/1287
BOUNDARY: 219.41725670417347| mean:214.45656363549116, std:19.842772274729224, z1:8.0, z2:0.25
* Evict result docs 678, queries 50, total 728
Document only #678/728
Query only #50/728
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([38, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([50, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1197 -> 678,queries# 90 -> 50, new std:20.04408317093872
Query only #59/1760
Document only #1701/1760
BOUNDARY: 222.3065716845946| mean:217.92547590339726, std:17.524383124789356, z1:8.0, z2:0.25
* Evict result docs 1002, queries 34, total 1036
Document only #1002/1036
Query only #34/1036
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([106, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([34, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1701 -> 1002,queries# 59 -> 34, new std:17.10574314871432
Query only #54/2057
Document only #2003/2057
BOUNDARY: 220.9038904778482| mean:216.70607737516627, std:16.791252410727672, z1:8.0, z2:0.25
* Evict result docs 1089, queries 29, total 1118
Document only #1089/1118
Query only #29/1118
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([65, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([29, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2003 -> 1089,queries# 54 -> 29, new std:16.32615028242681
clear_unused_documents | before total #11666
clear_unused_documents | after total #6729
############################################Eviction(21.091333389282227sec)############################################
Training Session 1/False
queries:400, docs:12062
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session1_docs.jsonl
queries:400, documents:12062
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12062 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:12062, document_embeddings:12062
Query-Document encoding ended.(21.852834701538086 sec.)
query_list:400, doc_list:12062
queries #400, documents #19191 initial_docs #0
#doc_stream 13, stream_docs size 174-1024
Session 1 | Document count:19191
############################################Initialize(22.209192276000977sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 2289
1th size: 2228
2th size: 836
3th size: 1192
4th size: 1208
Assign 0th stream ended(1.9631061553955078sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(1.9537129402160645sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.209141969680786sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.177806854248047sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.0869922637939453sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.205049514770508sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(2.1691651344299316sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.3061957359313965sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(2.339165687561035sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(2.2373929023742676sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.275412082672119sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(2.2817020416259766sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 46 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.3759329319000244sec).
############################################Assign(26.58077621459961sec)############################################
Document only #5202/5421
Query only #219/5421
Document only #5904/6086
Query only #182/6086
Document only #2066/2134
Query only #68/2134
Document only #2790/2889
Query only #99/2889
Document only #2599/2661
Query only #62/2661
Session 1 | SSE: 902352365.0
Document only #5202/5421
Document only #5904/6086
Document only #2066/2134
Document only #2790/2889
Document only #2599/2661
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #219/5421
Document only #5202/5421
BOUNDARY: 222.51654908180817| mean:218.83266064932022, std:14.735553729951745, z1:8.0, z2:0.25
* Evict result docs 3284, queries 138, total 3422
Document only #3284/3422
Query only #138/3422
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([84, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([10, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 5202 -> 3284,queries# 219 -> 138, new std:14.36727926305317
Query only #182/6086
Document only #5904/6086
BOUNDARY: 222.2101126335733| mean:218.46039056840635, std:14.998888260667773, z1:8.0, z2:0.25
* Evict result docs 3326, queries 102, total 3428
Document only #3326/3428
Query only #102/3428
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([126, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([102, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 5904 -> 3326,queries# 182 -> 102, new std:14.589838145486931
Query only #68/2134
Document only #2066/2134
BOUNDARY: 215.01703948318965| mean:210.2901870278186, std:18.907409821484205, z1:8.0, z2:0.25
* Evict result docs 1083, queries 35, total 1118
Document only #1083/1118
Query only #35/1118
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([59, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([35, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2066 -> 1083,queries# 68 -> 35, new std:16.818715840542378
Query only #99/2889
Document only #2790/2889
BOUNDARY: 219.02387456408283| mean:214.45818664879897, std:18.2627516611354, z1:8.0, z2:0.25
* Evict result docs 1615, queries 57, total 1672
Document only #1615/1672
Query only #57/1672
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([79, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([57, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2790 -> 1615,queries# 99 -> 57, new std:17.667432566808053
Query only #62/2661
Document only #2599/2661
BOUNDARY: 216.63833792016527| mean:212.3004192909053, std:17.35167451703994, z1:8.0, z2:0.25
* Evict result docs 1365, queries 32, total 1397
Document only #1365/1397
Query only #32/1397
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([85, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([32, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2599 -> 1365,queries# 62 -> 32, new std:15.904115836294547
clear_unused_documents | before total #19191
clear_unused_documents | after total #11037
############################################Eviction(33.70763039588928sec)############################################
Training Session 2/False
queries:400, docs:12625
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session2_docs.jsonl
queries:400, documents:12625
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12625 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12625, document_embeddings:12625
Query-Document encoding ended.(23.894309520721436 sec.)
query_list:400, doc_list:12625
queries #400, documents #24062 initial_docs #0
#doc_stream 13, stream_docs size 737-1024
Session 2 | Document count:24062
############################################Initialize(24.211755752563477sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 3599
1th size: 3679
2th size: 1658
3th size: 1696
4th size: 1429
Assign 0th stream ended(2.2697441577911377sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.133298397064209sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.2871217727661133sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.1958398818969727sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.2971267700195312sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.3522002696990967sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(2.3143186569213867sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.4547507762908936sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(2.575443744659424sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(2.4276673793792725sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.6865546703338623sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(2.7463185787200928sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 97 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(1.9520368576049805sec).
############################################Assign(30.692421913146973sec)############################################
Document only #5470/5679
Query only #209/5679
Document only #6586/6829
Query only #243/6829
Document only #7335/7553
Query only #218/7553
Document only #2039/2100
Query only #61/2100
Document only #1868/1901
Query only #33/1901
Session 2 | SSE: 1091165119.25
Document only #5470/5679
Document only #6586/6829
Document only #7335/7553
Document only #2039/2100
Document only #1868/1901
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #209/5679
Document only #5470/5679
BOUNDARY: 219.38581568280932| mean:215.64363944820167, std:14.968704938430648, z1:8.0, z2:0.25
* Evict result docs 3231, queries 123, total 3354
Document only #3231/3354
Query only #123/3354
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([31, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([123, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 5470 -> 3231,queries# 209 -> 123, new std:14.874795795243664
Query only #243/6829
Document only #6586/6829
BOUNDARY: 219.45704755790308| mean:215.51312433460095, std:15.775692893208486, z1:8.0, z2:0.25
* Evict result docs 3743, queries 138, total 3881
Document only #3743/3881
Query only #138/3881
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([31, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([10, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 6586 -> 3743,queries# 243 -> 138, new std:15.053383562299414
Query only #218/7553
Document only #7335/7553
BOUNDARY: 215.30612527179403| mean:210.6176642302439, std:18.753844166200576, z1:8.0, z2:0.25
* Evict result docs 3804, queries 113, total 3917
Document only #3804/3917
Query only #113/3917
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([92, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([113, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 7335 -> 3804,queries# 218 -> 113, new std:17.173779409799643
Query only #61/2100
Document only #2039/2100
BOUNDARY: 211.25122882589832| mean:206.65608855133965, std:18.380561098234747, z1:8.0, z2:0.25
* Evict result docs 1075, queries 32, total 1107
Document only #1075/1107
Query only #32/1107
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([51, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([32, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2039 -> 1075,queries# 61 -> 32, new std:17.605828970127263
Query only #33/1901
Document only #1868/1901
BOUNDARY: 207.26974333580276| mean:202.9461513573969, std:17.294367913623443, z1:8.0, z2:0.25
* Evict result docs 1070, queries 18, total 1088
Document only #1070/1088
Query only #18/1088
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([46, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([18, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1868 -> 1070,queries# 33 -> 18, new std:14.420766865420935
clear_unused_documents | before total #24062
clear_unused_documents | after total #13347
############################################Eviction(42.661229610443115sec)############################################
Training Session 3/False
queries:400, docs:13375
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session3_docs.jsonl
queries:400, documents:13375
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 13375 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:13375, document_embeddings:13375
Query-Document encoding ended.(24.661777019500732 sec.)
query_list:400, doc_list:13375
queries #400, documents #27122 initial_docs #0
#doc_stream 14, stream_docs size 463-1024
Session 3 | Document count:27122
############################################Initialize(24.97937798500061sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 3545
1th size: 4020
2th size: 4560
3th size: 1120
4th size: 1126
Assign 0th stream ended(2.3211276531219482sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.3398702144622803sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.574553966522217sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.5061793327331543sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.554114818572998sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.81949782371521sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(2.7255191802978516sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.769470691680908sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(2.947286367416382sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.158730983734131sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.2859554290771484sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.011774778366089sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(3.1181106567382812sec).
Assign 13th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 79 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 13th stream ended(1.4281296730041504sec).
############################################Assign(37.56032156944275sec)############################################
Document only #5904/6132
Query only #228/6132
Document only #5555/5742
Query only #187/5742
Document only #11862/12217
Query only #355/12217
Document only #1279/1313
Query only #34/1313
Document only #1698/1718
Query only #20/1718
Session 3 | SSE: 1183680060.25
Document only #5904/6132
Document only #5555/5742
Document only #11862/12217
Document only #1279/1313
Document only #1698/1718
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #228/6132
Document only #5904/6132
BOUNDARY: 216.95816187727777| mean:212.99163896436406, std:15.866091651654866, z1:8.0, z2:0.25
* Evict result docs 3513, queries 135, total 3648
Document only #3513/3648
Query only #135/3648
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([57, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([7, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 5904 -> 3513,queries# 228 -> 135, new std:15.356010009192472
Query only #187/5742
Document only #5555/5742
BOUNDARY: 214.4087696892205| mean:210.3423028207747, std:16.26586747378315, z1:8.0, z2:0.25
* Evict result docs 3161, queries 106, total 3267
Document only #3161/3267
Query only #106/3267
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([89, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([106, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 5555 -> 3161,queries# 187 -> 106, new std:14.898985988847237
Query only #355/12217
Document only #11862/12217
BOUNDARY: 212.78454909541537| mean:207.98106905789933, std:19.213920150064165, z1:8.0, z2:0.25
* Evict result docs 6424, queries 192, total 6616
Document only #6424/6616
Query only #192/6616
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([24, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([64, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 11862 -> 6424,queries# 355 -> 192, new std:16.92998953856267
Query only #34/1313
Document only #1279/1313
BOUNDARY: 201.88443688038058| mean:197.105113252464, std:19.11729451166628, z1:8.0, z2:0.25
* Evict result docs 777, queries 20, total 797
Document only #777/797
Query only #20/797
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([9, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([20, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1279 -> 777,queries# 34 -> 20, new std:17.377599165010714
Query only #20/1718
Document only #1698/1718
BOUNDARY: 196.56068591180346| mean:192.53249910449537, std:16.112747229232347, z1:8.0, z2:0.25
* Evict result docs 1033, queries 12, total 1045
Document only #1033/1045
Query only #12/1045
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([9, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([12, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1698 -> 1033,queries# 20 -> 12, new std:13.126544823943666
clear_unused_documents | before total #27122
clear_unused_documents | after total #15373
############################################Eviction(50.708388805389404sec)############################################
Training Session 4/False
queries:400, docs:12179
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session4_docs.jsonl
queries:400, documents:12179
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12179 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:12179, document_embeddings:12179
Query-Document encoding ended.(21.91426992416382 sec.)
query_list:400, doc_list:12179
queries #400, documents #27952 initial_docs #0
#doc_stream 13, stream_docs size 291-1024
Session 4 | Document count:27952
############################################Initialize(22.40052342414856sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 3983
1th size: 3552
2th size: 6914
3th size: 822
4th size: 1126
Assign 0th stream ended(2.351172924041748sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.5369062423706055sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.5433566570281982sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.5503180027008057sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.830329418182373sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(2.7465858459472656sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.42128586769104sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.7893574237823486sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.6788828372955322sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.756497383117676sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(2.874697685241699sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.828326940536499sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 35 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(1.248615026473999sec).
############################################Assign(37.15633225440979sec)############################################
Document only #7486/7827
Query only #341/7827
Document only #6590/6811
Query only #221/6811
Document only #9998/10267
Query only #269/10267
Document only #1039/1060
Query only #21/1060
Document only #1974/1987
Query only #13/1987
Session 4 | SSE: 1191919952.0625
Document only #7486/7827
Document only #6590/6811
Document only #9998/10267
Document only #1039/1060
Document only #1974/1987
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #341/7827
Document only #7486/7827
BOUNDARY: 217.40672325854322| mean:213.15191666047602, std:17.019226392268866, z1:8.0, z2:0.25
* Evict result docs 4871, queries 221, total 5092
Document only #4871/5092
Query only #221/5092
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([7, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([93, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 7486 -> 4871,queries# 341 -> 221, new std:15.861670335912267
Query only #221/6811
Document only #6590/6811
BOUNDARY: 214.9277719308646| mean:210.4963730089886, std:17.72559568750396, z1:8.0, z2:0.25
* Evict result docs 4155, queries 139, total 4294
Document only #4155/4294
Query only #139/4294
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([59, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([11, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 6590 -> 4155,queries# 221 -> 139, new std:14.90425866849382
Query only #269/10267
Document only #9998/10267
BOUNDARY: 205.82551086588768| mean:201.0168432010884, std:19.234670659197107, z1:8.0, z2:0.25
* Evict result docs 6092, queries 163, total 6255
Document only #6092/6255
Query only #163/6255
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([76, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([35, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9998 -> 6092,queries# 269 -> 163, new std:15.895838688498825
Query only #21/1060
Document only #1039/1060
BOUNDARY: 198.7729089639448| mean:193.54337176241964, std:20.918148806100586, z1:8.0, z2:0.25
* Evict result docs 731, queries 14, total 745
Document only #731/745
Query only #14/745
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([91, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([14, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1039 -> 731,queries# 21 -> 14, new std:17.047013704181595
Query only #13/1987
Document only #1974/1987
BOUNDARY: 193.2284309643938| mean:188.91933280376554, std:17.236392642513067, z1:8.0, z2:0.25
* Evict result docs 1336, queries 8, total 1344
Document only #1336/1344
Query only #8/1344
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([56, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([8, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1974 -> 1336,queries# 13 -> 8, new std:11.913847251849344
clear_unused_documents | before total #27952
clear_unused_documents | after total #17730
############################################Eviction(80.8960611820221sec)############################################
Training Session 5/False
queries:400, docs:11993
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session5_docs.jsonl
queries:400, documents:11993
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 11993 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:11993, document_embeddings:11993
Query-Document encoding ended.(21.196839809417725 sec.)
query_list:400, doc_list:11993
queries #400, documents #30123 initial_docs #0
#doc_stream 13, stream_docs size 105-1024
Session 5 | Document count:30123
############################################Initialize(21.511801958084106sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 5488
1th size: 4635
2th size: 6334
3th size: 875
4th size: 1422
Assign 0th stream ended(3.1823863983154297sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(3.0556368827819824sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(3.240617275238037sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(3.469412088394165sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(3.5594208240509033sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(4.439236164093018sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(4.9927685260772705sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(3.7302920818328857sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(4.088045597076416sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.909459114074707sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(4.651923179626465sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(5.198886156082153sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 105 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.553093433380127sec).
############################################Assign(48.07117772102356sec)############################################
Document only #9425/9867
Query only #442/9867
Document only #8238/8477
Query only #239/8477
Document only #6977/7143
Query only #166/7143
Document only #2525/2613
Query only #88/2613
Document only #2013/2023
Query only #10/2023
Session 5 | SSE: 1284569097.9375
Document only #9425/9867
Document only #8238/8477
Document only #6977/7143
Document only #2525/2613
Document only #2013/2023
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #442/9867
Document only #9425/9867
BOUNDARY: 217.48145920674077| mean:213.19788211611274, std:17.13430836251216, z1:8.0, z2:0.25
* Evict result docs 6342, queries 297, total 6639
Document only #6342/6639
Query only #297/6639
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([70, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([41, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9425 -> 6342,queries# 442 -> 297, new std:15.842768121279303
Query only #239/8477
Document only #8238/8477
BOUNDARY: 215.31480367459343| mean:210.9129404776555, std:17.607452787751807, z1:8.0, z2:0.25
* Evict result docs 5176, queries 150, total 5326
Document only #5176/5326
Query only #150/5326
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([56, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([22, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 8238 -> 5176,queries# 239 -> 150, new std:14.420255534006284
Query only #166/7143
Document only #6977/7143
BOUNDARY: 197.11685285339317| mean:192.74028851724583, std:17.506257344589343, z1:8.0, z2:0.25
* Evict result docs 4621, queries 109, total 4730
Document only #4621/4730
Query only #109/4730
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([13, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([109, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 6977 -> 4621,queries# 166 -> 109, new std:15.001426671353201
Query only #88/2613
Document only #2525/2613
BOUNDARY: 214.7997567916116| mean:209.0416212482467, std:23.0325421734595, z1:8.0, z2:0.25
* Evict result docs 1330, queries 46, total 1376
Document only #1330/1376
Query only #46/1376
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([50, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([46, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2525 -> 1330,queries# 88 -> 46, new std:19.402959094198124
Query only #10/2023
Document only #2013/2023
BOUNDARY: 190.09144900935655| mean:186.01306264540727, std:16.313545455797136, z1:8.0, z2:0.25
* Evict result docs 1381, queries 6, total 1387
Document only #1381/1387
Query only #6/1387
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([101, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([6, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2013 -> 1381,queries# 10 -> 6, new std:11.03195094677545
clear_unused_documents | before total #30123
clear_unused_documents | after total #19458
############################################Eviction(80.37087273597717sec)############################################
Training Session 6/False
queries:400, docs:12039
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session6_docs.jsonl
queries:400, documents:12039
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12039 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
new_d_data | new_d_data:12039, document_embeddings:12039
Query-Document encoding ended.(21.308024883270264 sec.)
query_list:400, doc_list:12039
queries #400, documents #31897 initial_docs #0
#doc_stream 13, stream_docs size 151-1024
Session 6 | Document count:31897
############################################Initialize(21.630293130874634sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 6945
1th size: 5765
2th size: 4792
3th size: 1542
4th size: 1438
Assign 0th stream ended(4.770509481430054sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(4.270657300949097sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(3.564574956893921sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(3.6375226974487305sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(4.049213409423828sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(4.282003879547119sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(4.020279884338379sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(4.206773519515991sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(4.252669811248779sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(4.482200860977173sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(4.6259753704071045sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.5129215717315674sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 23 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(0.5314078330993652sec).
############################################Assign(50.20671057701111sec)############################################
Document only #10090/10539
Query only #449/10539
Document only #10142/10464
Query only #322/10464
Document only #5435/5552
Query only #117/5552
Document only #3281/3393
Query only #112/3393
Document only #1941/1949
Query only #8/1949
Session 6 | SSE: 1360734933.25
Document only #10090/10539
Document only #10142/10464
Document only #5435/5552
Document only #3281/3393
Document only #1941/1949
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #449/10539
Document only #10090/10539
BOUNDARY: 216.03031214031174| mean:211.78926270852358, std:16.964197727152587, z1:8.0, z2:0.25
* Evict result docs 6634, queries 295, total 6929
Document only #6634/6929
Query only #295/6929
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([106, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([39, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 10090 -> 6634,queries# 449 -> 295, new std:15.762112340265961
Query only #322/10464
Document only #10142/10464
BOUNDARY: 215.51338199826543| mean:211.15402022303746, std:17.437447100911854, z1:8.0, z2:0.25
* Evict result docs 6401, queries 203, total 6604
Document only #6401/6604
Query only #203/6604
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([1, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([75, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 10142 -> 6401,queries# 322 -> 203, new std:14.504633244369499
Query only #117/5552
Document only #5435/5552
BOUNDARY: 193.60837474196632| mean:189.06149275184364, std:18.187527960490687, z1:8.0, z2:0.25
* Evict result docs 3996, queries 86, total 4082
Document only #3996/4082
Query only #86/4082
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([28, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([86, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 5435 -> 3996,queries# 117 -> 86, new std:14.679121952811723
Query only #112/3393
Document only #3281/3393
BOUNDARY: 213.81995786918105| mean:208.27956360559213, std:22.16157705435573, z1:8.0, z2:0.25
* Evict result docs 1854, queries 63, total 1917
Document only #1854/1917
Query only #63/1917
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([62, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([63, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 3281 -> 1854,queries# 112 -> 63, new std:18.831690981705762
Query only #8/1949
Document only #1941/1949
BOUNDARY: 187.37158594539656| mean:183.49408436531039, std:15.51000632034475, z1:8.0, z2:0.25
* Evict result docs 1364, queries 5, total 1369
Document only #1364/1369
Query only #5/1369
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([84, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([5, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1941 -> 1364,queries# 8 -> 5, new std:10.619994034170247
clear_unused_documents | before total #31897
clear_unused_documents | after total #20901
############################################Eviction(89.40432667732239sec)############################################
Training Session 7/False
queries:400, docs:12414
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session7_docs.jsonl
queries:400, documents:12414
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12414 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12414, document_embeddings:12414
Query-Document encoding ended.(23.540311574935913 sec.)
query_list:400, doc_list:12414
queries #400, documents #33715 initial_docs #0
#doc_stream 13, stream_docs size 526-1024
Session 7 | Document count:33715
############################################Initialize(23.993115186691284sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 7125
1th size: 6897
2th size: 4536
3th size: 1963
4th size: 1404
Assign 0th stream ended(3.5874269008636475sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(3.7453665733337402sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(3.9269955158233643sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(3.8473012447357178sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(4.019589900970459sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(5.019389390945435sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.7740795612335205sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(4.653305292129517sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.1862382888793945sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.829339027404785sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(4.150390863418579sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.9315345287323sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 14 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(1.7119309902191162sec).
############################################Assign(49.382888078689575sec)############################################
Document only #9039/9400
Query only #361/9400
Document only #9887/10248
Query only #361/10248
Document only #9550/9810
Query only #260/9810
Document only #2339/2404
Query only #65/2404
Document only #1848/1853
Query only #5/1853
Session 7 | SSE: 1416082385.28125
Document only #9039/9400
Document only #9887/10248
Document only #9550/9810
Document only #2339/2404
Document only #1848/1853
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #361/9400
Document only #9039/9400
BOUNDARY: 212.80929013808054| mean:208.6286001308294, std:16.722760029004597, z1:8.0, z2:0.25
* Evict result docs 5626, queries 224, total 5850
Document only #5626/5850
Query only #224/5850
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([122, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([96, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9039 -> 5626,queries# 361 -> 224, new std:15.535770855176587
Query only #361/10248
Document only #9887/10248
BOUNDARY: 213.18972760764322| mean:208.86247461048958, std:17.30901198861456, z1:8.0, z2:0.25
* Evict result docs 6517, queries 237, total 6754
Document only #6517/6754
Query only #237/6754
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([117, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([109, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9887 -> 6517,queries# 361 -> 237, new std:14.719917042399095
Query only #260/9810
Document only #9550/9810
BOUNDARY: 205.79004627389602| mean:200.16131651891237, std:22.514919019934577, z1:8.0, z2:0.25
* Evict result docs 5591, queries 152, total 5743
Document only #5591/5743
Query only #152/5743
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([87, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([24, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9550 -> 5591,queries# 260 -> 152, new std:16.06235829203759
Query only #65/2404
Document only #2339/2404
BOUNDARY: 203.301763863673| mean:198.146917809365, std:20.61938421723202, z1:8.0, z2:0.25
* Evict result docs 1476, queries 41, total 1517
Document only #1476/1517
Query only #41/1517
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([68, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([41, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2339 -> 1476,queries# 65 -> 41, new std:17.68228909771482
Query only #5/1853
Document only #1848/1853
BOUNDARY: 184.17703435046832| mean:180.61180122723403, std:14.260932492937155, z1:8.0, z2:0.25
* Evict result docs 1224, queries 3, total 1227
Document only #1224/1227
Query only #3/1227
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([72, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([3, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1848 -> 1224,queries# 5 -> 3, new std:10.194944439920418
clear_unused_documents | before total #33715
clear_unused_documents | after total #21091
############################################Eviction(97.45400357246399sec)############################################
Training Session 8/False
queries:400, docs:12789
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session8_docs.jsonl
queries:400, documents:12789
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12789 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12789, document_embeddings:12789
Query-Document encoding ended.(23.45392370223999 sec.)
query_list:400, doc_list:12789
queries #400, documents #34280 initial_docs #0
#doc_stream 13, stream_docs size 901-1024
Session 8 | Document count:34280
############################################Initialize(23.883119821548462sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 6058
1th size: 6907
2th size: 6350
3th size: 1533
4th size: 1267
Assign 0th stream ended(3.70768666267395sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(4.153388977050781sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(3.3677239418029785sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(3.6032700538635254sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(3.055821418762207sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(3.9683778285980225sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.9463419914245605sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(3.6715400218963623sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.593656539916992sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.829167127609253sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.817516803741455sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.997544765472412sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 5 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(3.1710031032562256sec).
############################################Assign(47.883039236068726sec)############################################
Document only #8356/8697
Query only #341/8697
Document only #8486/8779
Query only #293/8779
Document only #12765/13139
Query only #374/13139
Document only #1685/1728
Query only #43/1728
Document only #1931/1937
Query only #6/1937
Session 8 | SSE: 1406318753.625
Document only #8356/8697
Document only #8486/8779
Document only #12765/13139
Document only #1685/1728
Document only #1931/1937
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #341/8697
Document only #8356/8697
BOUNDARY: 211.1706626310473| mean:206.7890244972381, std:17.52655253523679, z1:8.0, z2:0.25
* Evict result docs 5605, queries 228, total 5833
Document only #5605/5833
Query only #228/5833
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([101, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([100, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 8356 -> 5605,queries# 341 -> 228, new std:15.632430690596303
Query only #293/8779
Document only #8486/8779
BOUNDARY: 208.89088517111279| mean:204.7338566064753, std:16.628114258549836, z1:8.0, z2:0.25
* Evict result docs 5649, queries 195, total 5844
Document only #5649/5844
Query only #195/5844
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([17, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([67, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 8486 -> 5649,queries# 293 -> 195, new std:14.387919955223662
Query only #374/13139
Document only #12765/13139
BOUNDARY: 206.16824557551192| mean:200.61096314194813, std:22.229129734255167, z1:8.0, z2:0.25
* Evict result docs 7690, queries 225, total 7915
Document only #7690/7915
Query only #225/7915
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([10, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([97, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 12765 -> 7690,queries# 374 -> 225, new std:16.572557570178482
Query only #43/1728
Document only #1685/1728
BOUNDARY: 194.9533459251022| mean:190.11815657059628, std:19.34075741802368, z1:8.0, z2:0.25
* Evict result docs 1122, queries 28, total 1150
Document only #1122/1150
Query only #28/1150
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([98, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([28, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1685 -> 1122,queries# 43 -> 28, new std:17.70373507755288
Query only #6/1937
Document only #1931/1937
BOUNDARY: 183.91585093611428| mean:180.1301129471105, std:15.142951956015192, z1:8.0, z2:0.25
* Evict result docs 1436, queries 4, total 1440
Document only #1436/1440
Query only #4/1440
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([28, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([4, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1931 -> 1436,queries# 6 -> 4, new std:10.168788478022211
clear_unused_documents | before total #34280
clear_unused_documents | after total #22182
############################################Eviction(109.97778034210205sec)############################################
Training Session 9/False
queries:400, docs:12532
Read from: /home/work/.default/huijeong/data/msmarco_session/train_session9_docs.jsonl
queries:400, documents:12532
Using 1 GPUs: [device(type='cuda', index=0)]
RandomProjectionLSH use_tensor_key True
Query-Document encoding started.
Starting on cuda:0 with 400 queries and 12532 documents (batch size 3072)
cuda:0 | Query encoding batch 0
new_q_data | new_q_data:400, query_embeddings:400
cuda:0 | Document encoding batch 0
cuda:0 | Document encoding batch 3072
cuda:0 | Document encoding batch 6144
cuda:0 | Document encoding batch 9216
cuda:0 | Document encoding batch 12288
new_d_data | new_d_data:12532, document_embeddings:12532
Query-Document encoding ended.(22.544678211212158 sec.)
query_list:400, doc_list:12532
queries #400, documents #35114 initial_docs #0
#doc_stream 13, stream_docs size 644-1024
Session 9 | Document count:35114
############################################Initialize(22.8943452835083sec)############################################
Assign 0th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
0th size: 6184
1th size: 6168
2th size: 8181
3th size: 1167
4th size: 1506
Assign 0th stream ended(2.787501573562622sec).
Assign 1th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 1th stream ended(2.720435380935669sec).
Assign 2th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 2th stream ended(2.880828857421875sec).
Assign 3th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 3th stream ended(2.961695671081543sec).
Assign 4th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 4th stream ended(2.966783046722412sec).
Assign 5th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 5th stream ended(3.4000484943389893sec).
Assign 6th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 6th stream ended(3.034947395324707sec).
Assign 7th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 7th stream ended(2.9627466201782227sec).
Assign 8th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 8th stream ended(3.0166115760803223sec).
Assign 9th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 9th stream ended(3.23870849609375sec).
Assign 10th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 10th stream ended(3.1021721363067627sec).
Assign 11th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 11th stream ended(3.476520538330078sec).
Assign 12th stream starts.
assign_instance_or_add_cluster started.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 128 starts on cuda:0.
ㄴ Batch 4 starts on cuda:0.
assign_instance_or_add_cluster finished.(5)
Assign 12th stream ended(2.019029378890991sec).
############################################Assign(38.568029165267944sec)############################################
Document only #9604/10026
Query only #422/10026
Document only #9383/9720
Query only #337/9720
Document only #11238/11526
Query only #288/11526
Document only #1425/1454
Query only #29/1454
Document only #2384/2388
Query only #4/2388
Session 9 | SSE: 1423105746.03125
Document only #9604/10026
Document only #9383/9720
Document only #11238/11526
Document only #1425/1454
Document only #2384/2388
Clear invalid clusters #5 -> #5
evict_cluster_instances started.
Query only #422/10026
Document only #9604/10026
BOUNDARY: 212.31100647721976| mean:207.67983650740288, std:18.524679879267506, z1:8.0, z2:0.25
* Evict result docs 6562, queries 288, total 6850
Document only #6562/6850
Query only #288/6850
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([34, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([32, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9604 -> 6562,queries# 422 -> 288, new std:15.938425663031225
Query only #337/9720
Document only #9383/9720
BOUNDARY: 210.69647584188615| mean:206.08467629765286, std:18.447198176933153, z1:8.0, z2:0.25
* Evict result docs 6551, queries 235, total 6786
Document only #6551/6786
Query only #235/6786
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([23, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([107, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 9383 -> 6551,queries# 337 -> 235, new std:14.598100625260908
Query only #288/11526
Document only #11238/11526
BOUNDARY: 199.44178376291023| mean:194.28067707550022, std:20.64442674963999, z1:8.0, z2:0.25
* Evict result docs 7853, queries 201, total 8054
Document only #7853/8054
Query only #201/8054
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([45, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([73, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 11238 -> 7853,queries# 288 -> 201, new std:15.518039081774754
Query only #29/1454
Document only #1425/1454
BOUNDARY: 194.007075961561| mean:188.57663127251143, std:21.721778756198244, z1:8.0, z2:0.25
* Evict result docs 1109, queries 22, total 1131
Document only #1109/1131
Query only #22/1131
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([85, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([22, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 1425 -> 1109,queries# 29 -> 22, new std:17.312832478636885
Query only #4/2388
Document only #2384/2388
BOUNDARY: 185.15210706459874| mean:181.16641673730246, std:15.94276130918511, z1:8.0, z2:0.25
* Evict result docs 1783, queries 2, total 1785
Document only #1783/1785
Query only #2/1785
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([128, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([119, 254, 768]), self.prototype:torch.Size([1, 8, 768])
update_statistics batch_token_embs:torch.Size([2, 254, 768]), self.prototype:torch.Size([1, 8, 768])
doc_ids# 2384 -> 1783,queries# 4 -> 2, new std:9.464799802769589
clear_unused_documents | before total #35114
clear_unused_documents | after total #24606
############################################Eviction(80.66314768791199sec)############################################
